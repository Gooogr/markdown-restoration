{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /raid/bsalyp/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from typing import List\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def get_token_labels(tokens: List[str], tag: str) -> List[str]:\n",
    "    seq_len = len(tokens)\n",
    "    if seq_len == 0:\n",
    "        return []\n",
    "    return [tag] + ['O']*(seq_len - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikipedia (/raid/bsalyp/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n",
      "100%|██████████| 10/10 [00:00<00:00, 46.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# load data and split by convenient chunks\n",
    "# take only part of data, 6.5M is too much anyway\n",
    "data = load_dataset(\"wikipedia\", \"20220301.en\", \n",
    "       split =[f'train[{k}:{k+10000}]' for k in range(0, 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General info\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Dataset({\n",
       "     features: ['id', 'url', 'title', 'text'],\n",
       "     num_rows: 10000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'url', 'title', 'text'],\n",
       "     num_rows: 10000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'url', 'title', 'text'],\n",
       "     num_rows: 10000\n",
       " })]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('General info')\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunck(data:Dataset )->pd.DataFrame:\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Split in subtexts\n",
    "    df['text'] = df['title'] + '\\n' + df['text']\n",
    "    df['subtexts'] = df['text'].str.split('\\n')\n",
    "    df = df.explode('subtexts')\n",
    "\n",
    "    # Get text features\n",
    "    df['sents_amount'] = df['subtexts'].apply(lambda x: len(sent_tokenize(x)))\n",
    "    df['start_with_space'] = df['subtexts'].apply(lambda x: x[0] == ' ' if x else False)\n",
    "    df = df[df['sents_amount'] != 0]\n",
    "\n",
    "    #Detect text parts\n",
    "    df['type'] = None\n",
    "\n",
    "    size_one = df['sents_amount'] == 1\n",
    "    space_start = df['start_with_space'] == True\n",
    "    is_title = df['subtexts'].isin(df['title'])\n",
    "\n",
    "    df.loc[size_one & (~space_start), 'type'] = 'H2'\n",
    "    df.loc[is_title, 'type'] = 'H1'\n",
    "    df.loc[space_start, 'type'] = 'LIST'\n",
    "    df['type'] = df['type'].fillna('BLOCK')\n",
    "\n",
    "    # Tokenize subtexts\n",
    "    df['tokens'] = df['subtexts'].apply(lambda x: word_tokenize(x))\n",
    "    # Create labels\n",
    "    df['token_labels'] = df.apply(lambda x: get_token_labels(x.tokens, x.type), axis=1)\n",
    "\n",
    "    # Aggregate results\n",
    "    result_df = df.groupby('url')[['tokens', 'token_labels']].sum()\n",
    "    result_df = result_df.explode(['tokens', 'token_labels'])\n",
    "    result_df = result_df[result_df['tokens'] != '']\n",
    "    result_df = result_df.dropna() #drom NaN's of URL rows\n",
    "\n",
    "    result_df = result_df.reset_index(drop=True)\n",
    "    # print(f'Total tokens amount: {len(result_df):,}')\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data_chank in enumerate(tqdm(data)):\n",
    "    tmp_df = process_chunck(data_chank)\n",
    "    tmp_df.to_csv(f'data/tmp_results/wiki_{idx}.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for path_name in Path('data/tmp_results').iterdir():\n",
    "    dfs.append(pd.read_csv(path_name, \n",
    "                           usecols=['tokens', 'token_labels'], \n",
    "                           engine='python'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>``</td>\n",
       "      <td>H1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Hello</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 tokens token_labels\n",
       "0           0     ``           H1\n",
       "1           1  Hello            O\n",
       "2           2      ,            O"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens amount in dataset\n",
      "404,780,955\n"
     ]
    }
   ],
   "source": [
    "print('Total tokens amount in dataset')\n",
    "print(f'{len(full_df):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 98%, 1%, 1% \n",
    "train, validate, test = np.split(full_df, [int(.98*len(full_df)), int(.99*len(full_df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 396,685,335\n",
      "Validation size: 4,047,810\n",
      "Test size: 4,047,810\n"
     ]
    }
   ],
   "source": [
    "print(f'Train size: {len(train):,}')\n",
    "print(f'Validation size: {len(validate):,}')\n",
    "print(f'Test size: {len(test):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/raid/bsalyp/retext/markdown-restoration_torch/prepare_hf_wiki_data.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdgx/raid/bsalyp/retext/markdown-restoration_torch/prepare_hf_wiki_data.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train\u001b[39m.\u001b[39;49mto_csv(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdata/markdown/train_wiki.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, index\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m'\u001b[39;49m, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdgx/raid/bsalyp/retext/markdown-restoration_torch/prepare_hf_wiki_data.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m validate\u001b[39m.\u001b[39mto_csv(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/markdown/validate_wiki.txt\u001b[39m\u001b[39m'\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, index\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdgx/raid/bsalyp/retext/markdown-restoration_torch/prepare_hf_wiki_data.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m test\u001b[39m.\u001b[39mto_csv(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/markdown/test_wiki.txt\u001b[39m\u001b[39m'\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, index\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/markdown_restoration/lib/python3.8/site-packages/pandas/core/generic.py:3551\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3540\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[1;32m   3542\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3543\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[1;32m   3544\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3548\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[1;32m   3549\u001b[0m )\n\u001b[0;32m-> 3551\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[1;32m   3552\u001b[0m     path_or_buf,\n\u001b[1;32m   3553\u001b[0m     line_terminator\u001b[39m=\u001b[39;49mline_terminator,\n\u001b[1;32m   3554\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[1;32m   3555\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   3556\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   3557\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   3558\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[1;32m   3559\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   3560\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[1;32m   3561\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   3562\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m   3563\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[1;32m   3564\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[1;32m   3565\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[1;32m   3566\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[1;32m   3567\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   3568\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/markdown_restoration/lib/python3.8/site-packages/pandas/io/formats/format.py:1180\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1162\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1163\u001b[0m     line_terminator\u001b[39m=\u001b[39mline_terminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[1;32m   1179\u001b[0m )\n\u001b[0;32m-> 1180\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[1;32m   1182\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1183\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/miniconda3/envs/markdown_restoration/lib/python3.8/site-packages/pandas/io/formats/csvs.py:261\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m    242\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilepath_or_buffer,\n\u001b[1;32m    243\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[1;32m    252\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[1;32m    253\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_terminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[0;32m--> 261\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save()\n",
      "File \u001b[0;32m~/miniconda3/envs/markdown_restoration/lib/python3.8/site-packages/pandas/io/formats/csvs.py:266\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_need_to_save_header:\n\u001b[1;32m    265\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_header()\n\u001b[0;32m--> 266\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_body()\n",
      "File \u001b[0;32m~/miniconda3/envs/markdown_restoration/lib/python3.8/site-packages/pandas/io/formats/csvs.py:304\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mif\u001b[39;00m start_i \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m end_i:\n\u001b[1;32m    303\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_chunk(start_i, end_i)\n",
      "File \u001b[0;32m~/miniconda3/envs/markdown_restoration/lib/python3.8/site-packages/pandas/io/formats/csvs.py:311\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    308\u001b[0m slicer \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(start_i, end_i)\n\u001b[1;32m    309\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc[slicer]\n\u001b[0;32m--> 311\u001b[0m res \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mto_native_types(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_number_format)\n\u001b[1;32m    312\u001b[0m data \u001b[39m=\u001b[39m [res\u001b[39m.\u001b[39miget_values(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(res\u001b[39m.\u001b[39mitems))]\n\u001b[1;32m    314\u001b[0m ix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_index[slicer]\u001b[39m.\u001b[39m_format_native_types(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_number_format)\n",
      "File \u001b[0;32m~/miniconda3/envs/markdown_restoration/lib/python3.8/site-packages/pandas/core/internals/managers.py:473\u001b[0m, in \u001b[0;36mBaseBlockManager.to_native_types\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_native_types\u001b[39m(\u001b[39mself\u001b[39m: T, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    469\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[39m    Convert values to native types (strings / python objects) that are used\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m    in formatting (repr / csv).\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\u001b[39m\"\u001b[39;49m\u001b[39mto_native_types\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/markdown_restoration/lib/python3.8/site-packages/pandas/core/internals/managers.py:304\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m         applied \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mapply(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    303\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m         applied \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(b, f)(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    305\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mNotImplementedError\u001b[39;00m):\n\u001b[1;32m    306\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_failures:\n",
      "File \u001b[0;32m~/miniconda3/envs/markdown_restoration/lib/python3.8/site-packages/pandas/core/internals/blocks.py:634\u001b[0m, in \u001b[0;36mBlock.to_native_types\u001b[0;34m(self, na_rep, quoting, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_native_types\u001b[39m(\u001b[39mself\u001b[39m, na_rep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnan\u001b[39m\u001b[39m\"\u001b[39m, quoting\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    633\u001b[0m     \u001b[39m\"\"\"convert to our native types format\"\"\"\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m     result \u001b[39m=\u001b[39m to_native_types(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalues, na_rep\u001b[39m=\u001b[39;49mna_rep, quoting\u001b[39m=\u001b[39;49mquoting, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    635\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_block(result)\n",
      "File \u001b[0;32m~/miniconda3/envs/markdown_restoration/lib/python3.8/site-packages/pandas/core/internals/blocks.py:2223\u001b[0m, in \u001b[0;36mto_native_types\u001b[0;34m(values, na_rep, quoting, float_format, decimal, **kwargs)\u001b[0m\n\u001b[1;32m   2219\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[1;32m   2221\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2223\u001b[0m     mask \u001b[39m=\u001b[39m isna(values)\n\u001b[1;32m   2224\u001b[0m     itemsize \u001b[39m=\u001b[39m writers\u001b[39m.\u001b[39mword_len(na_rep)\n\u001b[1;32m   2226\u001b[0m     \u001b[39mif\u001b[39;00m values\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m _dtype_obj \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m quoting \u001b[39mand\u001b[39;00m itemsize:\n",
      "File \u001b[0;32m~/miniconda3/envs/markdown_restoration/lib/python3.8/site-packages/pandas/core/dtypes/missing.py:143\u001b[0m, in \u001b[0;36misna\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39misna\u001b[39m(obj):\n\u001b[1;32m     67\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m    Detect missing values for an array-like object.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39m    Name: 1, dtype: bool\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m _isna(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/markdown_restoration/lib/python3.8/site-packages/pandas/core/dtypes/missing.py:172\u001b[0m, in \u001b[0;36m_isna\u001b[0;34m(obj, inf_as_na)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, (np\u001b[39m.\u001b[39mndarray, ABCExtensionArray)):\n\u001b[0;32m--> 172\u001b[0m     \u001b[39mreturn\u001b[39;00m _isna_array(obj, inf_as_na\u001b[39m=\u001b[39;49minf_as_na)\n\u001b[1;32m    173\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, ABCIndex):\n\u001b[1;32m    174\u001b[0m     \u001b[39m# Try to use cached isna, which also short-circuits for integer dtypes\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[39m#  and avoids materializing RangeIndex._values\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m obj\u001b[39m.\u001b[39m_can_hold_na:\n",
      "File \u001b[0;32m~/miniconda3/envs/markdown_restoration/lib/python3.8/site-packages/pandas/core/dtypes/missing.py:254\u001b[0m, in \u001b[0;36m_isna_array\u001b[0;34m(values, inf_as_na)\u001b[0m\n\u001b[1;32m    252\u001b[0m         result \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39misna()  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[39melif\u001b[39;00m is_string_or_object_np_dtype(values\u001b[39m.\u001b[39mdtype):\n\u001b[0;32m--> 254\u001b[0m     result \u001b[39m=\u001b[39m _isna_string_dtype(values, inf_as_na\u001b[39m=\u001b[39;49minf_as_na)\n\u001b[1;32m    255\u001b[0m \u001b[39melif\u001b[39;00m needs_i8_conversion(dtype):\n\u001b[1;32m    256\u001b[0m     \u001b[39m# this is the NaT pattern\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     result \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mview(\u001b[39m\"\u001b[39m\u001b[39mi8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m iNaT\n",
      "File \u001b[0;32m~/miniconda3/envs/markdown_restoration/lib/python3.8/site-packages/pandas/core/dtypes/missing.py:278\u001b[0m, in \u001b[0;36m_isna_string_dtype\u001b[0;34m(values, inf_as_na)\u001b[0m\n\u001b[1;32m    276\u001b[0m     result \u001b[39m=\u001b[39m libmissing\u001b[39m.\u001b[39misnaobj(values, inf_as_na\u001b[39m=\u001b[39minf_as_na)\n\u001b[1;32m    277\u001b[0m \u001b[39melif\u001b[39;00m values\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m--> 278\u001b[0m     result \u001b[39m=\u001b[39m libmissing\u001b[39m.\u001b[39;49misnaobj2d(values, inf_as_na\u001b[39m=\u001b[39;49minf_as_na)\n\u001b[1;32m    279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    280\u001b[0m     \u001b[39m# 0-D, reached via e.g. mask_missing\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     result \u001b[39m=\u001b[39m libmissing\u001b[39m.\u001b[39misnaobj(values\u001b[39m.\u001b[39mravel(), inf_as_na\u001b[39m=\u001b[39minf_as_na)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train.to_csv(r'data/markdown/train_wiki.txt', header=None, index=None, sep='\\t', mode='w')\n",
    "validate.to_csv(r'data/markdown/validate_wiki.txt', header=None, index=None, sep='\\t', mode='w')\n",
    "test.to_csv(r'data/markdown/test_wiki.txt', header=None, index=None, sep='\\t', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('markdown_restoration')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed90bc75d2a7c0132f165056d21b32dbed65ea36550be25cad2b58605b6f3df2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
